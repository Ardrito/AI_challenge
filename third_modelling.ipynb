{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e3026e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torchvision.transforms as transformers\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm.notebook import tqdm\n",
    "import sklearn\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "\n",
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import math\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5c6da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "omni2_scaler = joblib.load(\"omni2_scaler.gz\") \n",
    "goes_scaler = joblib.load(\"goes_scaler.gz\") \n",
    "initial_states_scaler = joblib.load(\"initial_states_scaler.gz\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab2ed623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.000e+03,  1.550e+02,  0.000e+00,  2.277e+03,  5.100e+01,\n",
       "        4.500e+01,  9.000e+00,  1.000e+00,  1.600e+00,  6.000e-01,\n",
       "       -8.510e+01,  4.000e-01, -1.660e+01, -1.740e+01, -4.520e+01,\n",
       "       -1.750e+01, -4.530e+01,  0.000e+00,  3.000e-01,  0.000e+00,\n",
       "        1.000e-01,  1.000e-01,  6.772e+03,  5.000e-01,  3.240e+02,\n",
       "       -1.300e+01, -1.050e+01,  8.000e-03,  0.000e+00,  0.000e+00,\n",
       "        0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  2.700e-01,\n",
       "       -2.296e+01,  0.000e+00,  1.200e+00,  1.200e+00,  1.200e-03,\n",
       "        3.000e+00,  1.130e+02, -3.000e+02,  2.000e+00,  1.524e+02,\n",
       "        3.000e+01, -1.164e+03, -1.500e+01, -7.700e+00,  8.363e-03,\n",
       "        3.600e-01,  3.400e-01,  3.400e-01,  3.200e-01,  1.800e-01,\n",
       "        1.000e-01, -1.000e+00])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omni2_scaler.data_min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b475574",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_states = []\n",
    "\n",
    "path = \"input_data/\"\n",
    "\n",
    "for dir, sub_dir, files in os.walk(path):\n",
    "    for file in sorted(files):\n",
    "        #print(file)\n",
    "        temp = pd.read_csv((path+file),index_col=None, header=0)\n",
    "        initial_states.append(temp)\n",
    "\n",
    "initial_states_df = pd.concat(initial_states,axis=0,ignore_index=True)\n",
    "\n",
    "initial_states_norm_df = np.where(initial_states_df.iloc[:,2:] > 1e+10,0.0, initial_states_df.iloc[:,2:])\n",
    "\n",
    "initial_states_scaler = MinMaxScaler()\n",
    "initial_states_scaler_values = initial_states_scaler.fit(initial_states_norm_df)\n",
    "\n",
    "initial_states_normalized = initial_states_scaler_values.transform(initial_states_df.iloc[:,2:].values)\n",
    "\n",
    "initial_states_normalized = np.where(initial_states_normalized >=1, 0.99,initial_states_normalized)\n",
    "\n",
    "initial_states_normalized = pd.concat([initial_states_df['File ID'],pd.DataFrame(initial_states_normalized)],axis=1)\n",
    "\n",
    "# hold = initial_states_df\n",
    "# x = initial_states_df.iloc[:,2:].values\n",
    "# x = normalize(x,norm='l2')\n",
    "# hold = pd.concat([hold['File ID'],pd.DataFrame(x)],axis=1)\n",
    "# initial_states_normalized = hold\n",
    "# #'2000-08-02 04:50:33'\n",
    "# timestamps = initial_states_df['Timestamp']\n",
    "# #initial_states_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e980523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullDatasetPT(Dataset):\n",
    "    def __init__(self, initial_states_df, pt_dir='data/new_processed/pt_files'):\n",
    "        self.data = initial_states_df.reset_index(drop=True)\n",
    "        self.pt_dir = pt_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        file_id = str(int(row['File ID'])).zfill(5)\n",
    "        pt_path = os.path.join(self.pt_dir, f\"{file_id}.pt\")\n",
    "\n",
    "        if not os.path.exists(pt_path):\n",
    "            raise FileNotFoundError(f\".pt file not found for File ID: {file_id}\")\n",
    "\n",
    "        static_input = torch.tensor(row.drop(\"File ID\").fillna(0.0).values, dtype=torch.float32)\n",
    "        pt_data = torch.load(pt_path)\n",
    "        \n",
    "        #print (pt_data)\n",
    "        return (\n",
    "            static_input,\n",
    "            pt_data[\"density\"],\n",
    "            pt_data[\"density_mask\"],\n",
    "            pt_data[\"goes\"],\n",
    "            pt_data[\"goes_mask\"],\n",
    "            pt_data[\"omni2\"],\n",
    "            pt_data[\"omni2_mask\"]\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbd2593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Positional Encoding for Sequences\n",
    "# -----------------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=4320):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(-torch.arange(0, d_model, 2) * math.log(10000.0) / d_model)\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "\n",
    "# -----------------------------------\n",
    "# STORMTransformer with Feature Mask Concatenation (No Downsampling)\n",
    "# -----------------------------------\n",
    "class STORMTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 static_dim=9,\n",
    "                 omni2_dim=57,\n",
    "                 goes_dim=42,\n",
    "                 d_model=128,\n",
    "                 output_len=432,\n",
    "                 nhead=8,\n",
    "                 num_layers=4,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.static_encoder = nn.Sequential(\n",
    "            nn.Linear(static_dim, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(d_model)\n",
    "        )\n",
    "\n",
    "        # Inputs are doubled due to feature-mask concatenation\n",
    "        self.omni2_proj = nn.Linear(omni2_dim * 2, d_model)\n",
    "        self.goes_proj = nn.Linear(goes_dim * 2, d_model)\n",
    "\n",
    "        self.omni2_pos = PositionalEncoding(d_model)\n",
    "        self.goes_pos = PositionalEncoding(d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.omni2_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.goes_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # self.fusion = nn.Sequential(\n",
    "        #     nn.Linear(d_model * 3, 256),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(dropout),\n",
    "        #     nn.Linear(256, output_len)\n",
    "        # )\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(d_model * 3, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256,360),\n",
    "            nn.BatchNorm1d(360),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(360, output_len)\n",
    "        )\n",
    "\n",
    "    def forward(self, static_input, omni2_seq, goes_seq, omni2_mask=None, goes_mask=None):\n",
    "        B = static_input.size(0)\n",
    "\n",
    "        # ----- Static Embedding -----\n",
    "        #print(\"static input\",static_input)\n",
    "        static_embed = self.static_encoder(static_input)\n",
    "        #print(\"static embed\", static_embed)\n",
    "\n",
    "        # ----- OMNI2 -----\n",
    "        if omni2_mask is not None:\n",
    "            omni2_cat = torch.cat([omni2_seq, omni2_mask], dim=-1)  # [B, T, 2D]\n",
    "        else:\n",
    "            omni2_cat = omni2_seq\n",
    "        omni2_embed = self.omni2_proj(omni2_cat)\n",
    "        omni2_embed = self.omni2_pos(omni2_embed)\n",
    "        omni2_out = self.omni2_encoder(omni2_embed)  # ⬅️ No key mask\n",
    "        omni2_summary = omni2_out.mean(dim=1)\n",
    "\n",
    "        # ----- GOES Downsampling to 8640 -----\n",
    "        if goes_seq.shape[1] > 4320:\n",
    "            step = goes_seq.shape[1] // 4320\n",
    "            goes_seq = goes_seq[:, ::step, :]\n",
    "            goes_mask = goes_mask[:, ::step, :] if goes_mask is not None else None\n",
    "\n",
    "        if goes_mask is not None:\n",
    "            goes_cat = torch.cat([goes_seq, goes_mask], dim=-1)  # [B, T, 2D]\n",
    "        else:\n",
    "            goes_cat = goes_seq\n",
    "        goes_embed = self.goes_proj(goes_cat)\n",
    "        goes_embed = self.goes_pos(goes_embed)\n",
    "        goes_out = self.goes_encoder(goes_embed)  # ⬅️ No key mask\n",
    "        goes_summary = goes_out.mean(dim=1)\n",
    "\n",
    "        # print(\"static\",static_embed)\n",
    "        # print(\"omni2\",omni2_summary)\n",
    "        # print(\"goes\",goes_summary)\n",
    "\n",
    "        # ----- Fusion -----\n",
    "        combined = torch.cat((static_embed, omni2_summary, goes_summary), dim=-1)\n",
    "        return self.fusion(combined)\n",
    "\n",
    "# -----------------------------------\n",
    "# Masked MSE Loss\n",
    "# -----------------------------------\n",
    "def masked_mse_loss(preds, targets, mask, eps=1e-8):\n",
    "    # preds = torch.nan_to_num(preds, nan=0.0, posinf=1e3, neginf=0.0)\n",
    "    # targets = torch.nan_to_num(targets, nan=0.0, posinf=1e3, neginf=0.0)\n",
    "    # loss = (preds - targets) ** 2 * mask\n",
    "    # return loss.sum() / (mask.sum() + eps)\n",
    "    diff = (targets - preds) * mask\n",
    "    sq = torch.square(diff)\n",
    "    sum = torch.sum(sq)\n",
    "    N = torch.sum(mask)\n",
    "    # print(sum)\n",
    "    # print(N)\n",
    "    loss = torch.sqrt((sum/N))\n",
    "    return loss\n",
    "\n",
    "# -----------------------------------\n",
    "# Full Training Loop with FullDataset\n",
    "# -----------------------------------\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c7e5459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_storm_transformer(initial_states_df, num_epochs=100, batch_size=16, lr=1e-3, device=None):\n",
    "    # from full_dataset import FullDataset\n",
    "    # from storm_transformer import STORMTransformer, masked_mse_loss\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # 🔀 Train/validation split\n",
    "    train_df, val_df = train_test_split(initial_states_df[0:8112], test_size=0.05, random_state=42)\n",
    "\n",
    "    train_dataset = FullDatasetPT(train_df)\n",
    "    val_dataset = FullDatasetPT(val_df)\n",
    "\n",
    "    # train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    # val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True, )\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "\n",
    "    model = STORMTransformer().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    checkpoint_path = \"checkpoints/storm_last.pt\"\n",
    "    best_model_path = \"checkpoints/storm_best.pt\"\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    # 🔁 Resume support\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"🔁 Resuming from checkpoint: {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint.get('epoch', 0)\n",
    "        best_val_loss = checkpoint.get('val_loss', float(\"inf\"))\n",
    "\n",
    "    # 🚀 Training loop\n",
    "    \n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        print(f\"\\n🚀 Epoch {epoch + 1}/{num_epochs}\")\n",
    "        #start_load = time.time()\n",
    "        for batch in tqdm(train_loader):\n",
    "            \n",
    "            static_input, density, density_mask, goes, goes_mask, omni2, omni2_mask = batch\n",
    "            #nd_load = time.time()\n",
    "            global omni2_out\n",
    "            omni2_out = omni2\n",
    "            \n",
    "            static_input = static_input.to(device)\n",
    "            density = density.to(device)\n",
    "            density_mask = density_mask.to(device)\n",
    "            goes = goes.to(device)\n",
    "            goes_mask = goes_mask.to(device)\n",
    "            omni2 = omni2.to(device)\n",
    "            omni2_mask = omni2_mask.to(device)\n",
    "            # print (\"static\",static_input)\n",
    "            # print (\"dens\",density)\n",
    "            # print (\"goes\",goes)\n",
    "            # print (\"omni2\",omni2)\n",
    "\n",
    "            # if (omni2_mask.any(dim=-1).sum(dim=1) == 0).any() or (goes_mask.any(dim=-1).sum(dim=1) == 0).any():\n",
    "            #     print(\"⚠️ Skipping batch with fully masked inputs\")\n",
    "            #     continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            preds = model(static_input, omni2, goes, omni2_mask, goes_mask)\n",
    "            # print (preds)\n",
    "            # print (\"------------------Preds--------------------\",len(preds))\n",
    "            # print (\"Preds:\", preds)\n",
    "            # print (\"Targets\",density)\n",
    "            #print (preds)\n",
    "            loss = masked_mse_loss(preds, density, density_mask)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            #end_batch = time.time()\n",
    "\n",
    "            # print (\"Load time:\", end_load - start_load )\n",
    "            # print (\"Calc time:\", end_batch - end_load)\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        print (\"Preds:\", preds)\n",
    "        print (\"Targets\",density)\n",
    "        \n",
    "        # 🧪 Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(val_loader):\n",
    "                static_input, density, density_mask, goes, goes_mask, omni2, omni2_mask = batch\n",
    "                \n",
    "\n",
    "                static_input = static_input.to(device)\n",
    "                density = density.to(device)\n",
    "                density_mask = density_mask.to(device)\n",
    "                goes = goes.to(device)\n",
    "                goes_mask = goes_mask.to(device)\n",
    "                omni2 = omni2.to(device)\n",
    "                omni2_mask = omni2_mask.to(device)\n",
    "\n",
    "                if (omni2_mask.any(dim=-1).sum(dim=1) == 0).any() or (goes_mask.any(dim=-1).sum(dim=1) == 0).any():\n",
    "                    continue\n",
    "\n",
    "                preds = model(static_input, omni2, goes, omni2_mask, goes_mask)\n",
    "                \n",
    "                loss = masked_mse_loss(preds, density, density_mask)\n",
    "                #print (\"loss\",loss,\"-------------------\")\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                # 🧠 Mask diagnostics\n",
    "                goes_mask_sum = goes_mask.sum().item()\n",
    "                omni2_mask_sum = omni2_mask.sum().item()\n",
    "                density_mask_sum = density_mask.sum().item()\n",
    "\n",
    "                print(f\"🧪 Eval Batch {batch_idx+1}/{len(val_loader)} — \"\n",
    "                      f\"OMNI2 Mask Sum: {omni2_mask_sum} | \"\n",
    "                      f\"GOES Mask Sum: {goes_mask_sum} | \"\n",
    "                      f\"Density Mask Sum: {density_mask_sum}\")\n",
    "\n",
    "                # ⚠️ Alert if any mask has < 10% coverage\n",
    "                if omni2_mask_sum < 0.1 * omni2_mask.numel():\n",
    "                    print(\"⚠️ Low OMNI2 coverage in this batch!\")\n",
    "                if goes_mask_sum < 0.1 * goes_mask.numel():\n",
    "                    print(\"⚠️ Low GOES coverage in this batch!\")\n",
    "                if density_mask_sum < 0.1 * density_mask.numel():\n",
    "                    print(\"⚠️ Low density mask coverage in this batch!\")\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        print(f\"\\n📊 Epoch {epoch+1}/{num_epochs} — \"\n",
    "              f\"Train Loss: {avg_train_loss} | Val Loss: {avg_val_loss}\")\n",
    "        \n",
    "        print (\"Preds:\", preds)\n",
    "        print (\"Targets\",density)\n",
    "        train_loss_history.append(avg_train_loss)\n",
    "        val_loss_history.append(avg_val_loss)\n",
    "\n",
    "        # 💾 Save full checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': avg_val_loss\n",
    "        }, checkpoint_path)\n",
    "\n",
    "        # 💎 Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), f\"checkpoints/epoch{epoch}.pt\")\n",
    "            print(\"✅ Best model updated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f393f7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 964/964 [10:04<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds: tensor([[ 3.0248e-03,  3.0156e-03, -4.4864e-03, -7.0914e-03,  2.5942e-03,\n",
      "         -1.4351e-03, -8.5881e-03, -7.0874e-03,  6.7298e-03,  8.8343e-03,\n",
      "          2.2860e-03,  7.1481e-03,  4.5089e-03,  5.3674e-03, -4.9099e-03,\n",
      "         -1.5461e-03, -3.3813e-03, -1.3004e-03, -3.8007e-03,  6.8478e-03,\n",
      "         -3.7238e-03,  1.1579e-03,  3.5341e-04, -5.4347e-03,  4.7645e-03,\n",
      "          3.6788e-03,  6.0750e-03, -2.3068e-04,  7.2794e-03,  5.4921e-04,\n",
      "         -2.6330e-03, -7.9644e-04, -3.2662e-03, -5.7969e-03,  1.3516e-03,\n",
      "         -1.4603e-03, -5.2927e-03,  9.1365e-03, -4.1576e-03, -1.3049e-03,\n",
      "          6.4635e-04,  3.4087e-03, -2.7705e-03, -4.7039e-05,  3.9814e-03,\n",
      "         -7.8685e-03,  6.4209e-03,  9.5020e-04, -1.7637e-03, -7.9494e-05,\n",
      "         -8.1475e-03, -2.5583e-03,  5.2639e-03,  6.3449e-03, -8.3751e-04,\n",
      "          9.5774e-04,  9.6082e-04,  1.6680e-03, -7.4036e-03,  4.2483e-03,\n",
      "         -6.9514e-03,  7.2583e-03, -3.4201e-03,  2.0080e-03,  2.3140e-03,\n",
      "         -4.2588e-03,  1.2134e-04, -1.5311e-03,  2.3674e-03,  2.0345e-03,\n",
      "         -2.8050e-04, -4.7982e-03,  7.8575e-03,  4.1091e-03,  7.7608e-04,\n",
      "         -7.8214e-03, -1.2203e-02,  5.3210e-04,  2.7841e-03,  6.7256e-03,\n",
      "         -3.5695e-03, -4.3254e-03, -2.9348e-03, -6.3519e-03, -2.1762e-03,\n",
      "         -2.9816e-03, -9.8762e-04, -2.1929e-03,  3.9463e-04, -2.6074e-03,\n",
      "          6.9898e-03, -5.3407e-04, -3.0399e-03,  3.8168e-03, -3.7718e-03,\n",
      "         -5.5738e-03,  2.1051e-03, -1.4780e-03,  4.1225e-04,  6.0148e-03,\n",
      "          1.5230e-03, -2.2378e-03,  7.2979e-04, -5.3180e-04, -6.6334e-03,\n",
      "         -6.8153e-03,  3.6292e-03,  1.9975e-03,  5.2482e-03,  2.8387e-03,\n",
      "          3.2190e-04,  8.4436e-04,  2.7208e-03, -1.1260e-02, -9.9651e-04,\n",
      "          4.7375e-03, -4.7994e-04, -1.8841e-03, -1.4658e-03,  3.7535e-03,\n",
      "         -5.2765e-03, -9.9459e-03,  4.3824e-03, -5.6720e-03,  5.0432e-03,\n",
      "         -2.8839e-03,  1.8960e-03, -2.0961e-03, -3.4363e-03, -4.0557e-03,\n",
      "          2.2423e-03, -4.7230e-03,  7.6297e-03,  1.3043e-02,  4.0613e-03,\n",
      "          1.2060e-04, -4.9907e-05,  5.3920e-03, -3.8313e-03,  4.5970e-04,\n",
      "         -2.5362e-03, -5.1362e-03, -1.9811e-03,  5.1663e-04,  1.3453e-03,\n",
      "         -2.7102e-03, -6.5751e-03,  2.5637e-03, -7.5785e-03,  1.0085e-02,\n",
      "          1.3509e-03, -3.9546e-03,  1.3948e-03,  5.7232e-04, -4.1566e-03,\n",
      "          9.4672e-03, -7.4363e-03, -4.9251e-03, -2.5974e-03, -3.8735e-03,\n",
      "          6.2638e-03, -8.0546e-04,  2.7451e-03, -2.1570e-03, -8.9436e-03,\n",
      "         -3.9600e-03, -2.1443e-03,  5.6691e-04, -9.6967e-05, -7.4706e-03,\n",
      "         -5.1386e-03, -6.4183e-04, -3.2694e-04,  1.1964e-03, -2.8353e-03,\n",
      "         -3.4429e-04,  8.5673e-03,  7.1648e-03,  1.1051e-03,  1.9528e-03,\n",
      "         -1.6749e-03, -5.0612e-03, -2.7406e-03, -3.5558e-04, -2.2821e-03,\n",
      "         -5.2487e-03,  7.0962e-04,  5.9992e-03,  3.1003e-03, -3.8121e-03,\n",
      "         -7.5333e-03,  8.8943e-03,  3.4623e-03, -1.1499e-02,  1.6965e-03,\n",
      "         -5.2183e-03,  6.8626e-03, -2.9464e-03, -3.4256e-03, -6.0413e-03,\n",
      "         -3.6239e-03, -1.0725e-02, -1.3640e-03, -9.2484e-03, -2.2030e-03,\n",
      "         -2.5686e-03, -1.5631e-03,  8.2385e-03,  8.0409e-03, -7.0932e-03,\n",
      "         -7.0702e-03, -9.4915e-03,  3.3708e-03,  3.5833e-03,  3.9371e-03,\n",
      "         -5.3096e-04, -5.2109e-03, -2.6012e-03,  6.1813e-03, -2.3920e-04,\n",
      "          1.4478e-03, -2.7496e-03,  2.9823e-03,  2.8496e-03,  6.2377e-03,\n",
      "         -4.4906e-03,  1.0531e-03, -1.8908e-03,  3.4032e-03, -7.2720e-03,\n",
      "         -5.2295e-03,  3.8851e-03,  3.5443e-03, -5.3185e-03, -2.7586e-03,\n",
      "         -1.0800e-03, -4.0597e-03,  6.2109e-03, -6.5427e-03,  9.8774e-03,\n",
      "          6.9695e-03, -4.1300e-03, -8.1143e-03, -7.0402e-03,  1.0861e-02,\n",
      "          3.7370e-03,  7.2462e-03, -5.4722e-03,  3.6122e-03,  1.5725e-03,\n",
      "         -5.2491e-03,  1.0938e-02,  8.8165e-03, -5.2613e-03, -4.8002e-03,\n",
      "          2.1040e-03,  6.5330e-03, -5.6133e-03,  1.9613e-03,  8.6766e-03,\n",
      "         -3.3955e-04,  4.6731e-03,  5.7199e-03,  1.1980e-03, -7.5925e-03,\n",
      "          5.1414e-03, -5.2396e-03,  1.4669e-03, -6.1387e-03, -2.5936e-03,\n",
      "          1.6791e-03,  5.3618e-03,  1.7429e-03, -1.5250e-02, -4.2242e-03,\n",
      "          7.9362e-03,  2.7697e-03, -3.2061e-03, -4.3078e-03,  7.8496e-03,\n",
      "          7.7985e-03, -3.3067e-03,  3.8171e-03,  2.9940e-03, -1.2092e-03,\n",
      "         -4.9511e-03, -1.6775e-03, -3.9699e-03, -3.9445e-03,  1.8175e-03,\n",
      "          2.1172e-03,  1.4876e-03, -1.0188e-02,  4.3073e-03, -2.1854e-03,\n",
      "         -1.9292e-03,  7.3194e-03,  2.7249e-04,  1.2656e-02,  1.3208e-03,\n",
      "         -5.2671e-03,  6.6914e-04,  4.0323e-03,  5.0726e-03,  1.9656e-03,\n",
      "          3.0172e-05, -6.9499e-03, -5.3444e-03,  2.1391e-03, -3.7768e-04,\n",
      "          1.1804e-03, -1.5870e-03, -6.0531e-03,  2.4668e-03, -9.1441e-03,\n",
      "          4.3278e-03,  4.0705e-03,  3.0406e-03,  2.2511e-04,  5.8668e-03,\n",
      "          3.7641e-03, -2.0269e-03,  3.0174e-03,  8.6545e-03,  1.3942e-03,\n",
      "          6.1627e-03,  9.0860e-03, -4.0652e-03, -3.5407e-03, -1.6447e-03,\n",
      "         -7.6459e-03, -1.0748e-03, -2.3214e-04,  6.6940e-04, -1.1169e-02,\n",
      "          4.1001e-03,  5.0232e-03,  6.3471e-03,  2.2728e-03, -2.2309e-03,\n",
      "         -2.2307e-03, -6.0652e-03,  9.4454e-05,  1.1003e-03, -3.5406e-03,\n",
      "         -2.9052e-03, -1.6267e-03, -2.3629e-03, -7.8639e-03, -2.1579e-03,\n",
      "         -6.6413e-03,  4.8893e-03, -5.6619e-03,  1.4495e-03,  6.0921e-04,\n",
      "          2.3822e-03,  2.4428e-03, -4.5289e-03, -4.7051e-03, -2.0275e-03,\n",
      "          6.3257e-03,  2.9864e-03, -1.2609e-03, -3.0509e-03,  2.8797e-03,\n",
      "         -2.2358e-04, -1.8076e-03, -8.5864e-03,  7.1500e-03, -2.8572e-03,\n",
      "         -1.8900e-03, -1.1200e-02,  2.0869e-04, -3.8377e-03, -2.7884e-03,\n",
      "         -6.1140e-03, -9.0747e-03,  1.7276e-03,  3.6654e-04, -1.5552e-03,\n",
      "          2.7404e-03, -5.7629e-04, -1.2573e-03, -4.7175e-03,  9.2329e-03,\n",
      "         -5.2644e-03,  5.4252e-04,  3.6569e-04, -5.1970e-03, -6.4906e-03,\n",
      "         -1.8324e-03, -1.3506e-02, -4.2600e-03,  1.0432e-02, -5.1445e-03,\n",
      "         -3.4557e-03, -3.0145e-03,  2.8846e-03,  4.4821e-03,  7.1164e-03,\n",
      "         -8.2068e-03,  3.4962e-03,  9.6910e-04,  5.6853e-03,  4.8484e-03,\n",
      "          2.7908e-03,  4.1916e-03,  3.8292e-03,  6.0028e-03, -4.8972e-03,\n",
      "          1.9059e-03, -7.7894e-03,  6.0360e-03, -1.0207e-02, -3.6980e-03,\n",
      "          6.7938e-03, -1.2205e-03,  9.2865e-04, -4.6307e-03, -5.2206e-03,\n",
      "         -3.0089e-03,  3.6365e-03,  1.3680e-03,  6.7903e-04,  7.1197e-03,\n",
      "         -1.4459e-02,  2.4184e-03, -6.3221e-03,  7.5588e-03, -6.5436e-03,\n",
      "         -6.1497e-03, -5.1474e-03],\n",
      "        [-2.9069e-04, -6.6851e-03,  1.6275e-03,  1.0273e-02, -4.6388e-03,\n",
      "         -9.2498e-03,  1.4149e-02,  3.5265e-03, -1.4630e-02, -4.2911e-03,\n",
      "         -3.1481e-03, -1.0460e-02, -2.3806e-02, -1.5398e-02,  2.4143e-02,\n",
      "          3.9465e-03,  5.7794e-03,  3.4252e-03,  8.4642e-03, -1.7448e-02,\n",
      "          2.0705e-02, -1.2782e-02, -2.4105e-03,  1.5379e-02, -5.2160e-05,\n",
      "         -1.5459e-02, -1.7552e-02,  6.6063e-03, -3.2332e-03, -1.4960e-02,\n",
      "          4.0932e-03, -2.7705e-03,  8.0778e-03,  3.8569e-03, -7.6511e-04,\n",
      "         -1.1327e-03,  9.2761e-03, -1.8742e-02,  1.0102e-02,  8.5049e-03,\n",
      "          2.4226e-03, -1.0221e-02,  9.1873e-03, -8.1525e-03, -8.7866e-03,\n",
      "          1.4575e-02, -2.6505e-03, -2.7189e-03,  7.1892e-03,  8.6346e-03,\n",
      "          1.2899e-02,  1.3616e-02, -4.4839e-03, -6.7074e-03, -2.4071e-03,\n",
      "         -5.5755e-03, -2.8795e-03,  6.0273e-03, -2.9362e-03, -6.4608e-05,\n",
      "          6.0717e-03, -6.2812e-03,  6.1059e-04, -8.7921e-04, -2.5642e-03,\n",
      "          2.5026e-03, -2.1843e-02, -5.0265e-03, -5.2682e-03, -6.4568e-04,\n",
      "          1.9655e-02,  5.7004e-03, -1.6221e-02, -8.6547e-03, -5.7252e-03,\n",
      "          7.1307e-03,  1.0342e-02,  6.0996e-03, -8.7858e-03, -8.4503e-03,\n",
      "          8.2513e-03,  2.3090e-02,  1.1091e-02,  1.1092e-02,  9.1349e-03,\n",
      "          6.7805e-03,  3.7913e-03,  1.4055e-02, -7.8205e-03,  6.4617e-03,\n",
      "         -1.6711e-02,  2.3992e-03,  2.1061e-03, -2.8641e-03,  1.3764e-03,\n",
      "          1.5108e-02, -1.1659e-02,  3.5209e-03, -1.7750e-03,  4.2993e-03,\n",
      "          5.8973e-03, -1.3541e-02, -4.8047e-03,  1.6600e-02,  1.0421e-02,\n",
      "          9.1794e-03, -8.8464e-03, -5.5586e-03, -1.2094e-02,  3.9431e-03,\n",
      "          6.1248e-03, -7.0291e-03, -1.8505e-03,  2.3166e-02, -1.1537e-03,\n",
      "         -9.1798e-03, -3.5303e-03,  7.5326e-03,  5.2779e-03, -9.6529e-04,\n",
      "          4.6748e-03,  2.3008e-02, -2.2285e-02,  3.3353e-03, -1.7791e-02,\n",
      "         -8.4976e-03, -3.3529e-03, -2.3292e-03,  6.5423e-03,  7.9509e-03,\n",
      "         -8.0157e-03,  9.6853e-03, -1.0369e-02, -2.3430e-02,  3.2979e-03,\n",
      "         -8.4347e-03, -9.1316e-03, -1.5351e-02,  1.6537e-03,  9.5884e-03,\n",
      "          1.0826e-02,  1.1523e-02, -2.3592e-03, -7.8503e-03, -1.2883e-02,\n",
      "          1.2090e-02,  1.9362e-02,  1.4642e-02,  1.6320e-02, -1.2532e-02,\n",
      "          8.5409e-03,  2.8366e-03,  1.0395e-03,  4.8611e-03, -3.7413e-03,\n",
      "         -1.4495e-02,  3.6236e-03,  1.2659e-03,  1.7997e-03,  1.8455e-03,\n",
      "         -1.2140e-02,  2.8083e-03,  7.8125e-03, -3.6908e-03,  1.1669e-02,\n",
      "          8.4199e-03,  1.3574e-02, -6.7936e-03,  1.1136e-02,  1.4132e-02,\n",
      "          2.4692e-03,  1.4887e-02,  1.4439e-02,  6.5026e-03,  5.6182e-03,\n",
      "          4.8847e-03, -1.5369e-02, -1.6431e-02,  4.0651e-03, -6.1098e-03,\n",
      "         -4.1314e-05,  1.1317e-02, -2.0837e-03, -2.1058e-04,  1.4714e-02,\n",
      "          1.7680e-02, -6.7194e-03, -6.4887e-03, -5.0772e-03,  4.6533e-03,\n",
      "          1.3794e-02, -1.8349e-02, -9.5276e-03,  2.0141e-02, -1.3272e-03,\n",
      "          1.5498e-02, -4.0163e-03,  6.0338e-03, -3.5100e-03,  2.5272e-04,\n",
      "          9.8341e-04,  1.9089e-02, -1.0077e-02,  9.2673e-03,  1.3077e-02,\n",
      "         -5.4747e-03,  1.2762e-03, -1.7535e-02, -3.1943e-02,  1.6398e-02,\n",
      "          2.2671e-02,  6.0127e-03, -4.9471e-03, -3.8061e-03, -1.7240e-02,\n",
      "         -9.6443e-03,  1.4811e-02,  1.0334e-02, -5.9764e-03,  5.4298e-03,\n",
      "          9.1046e-03,  4.1978e-03, -2.1433e-02, -1.1220e-02, -1.4499e-02,\n",
      "          6.2824e-03, -7.8883e-03, -5.1967e-03, -3.1723e-03, -7.2049e-03,\n",
      "          1.8212e-02, -1.4203e-02,  1.4066e-03,  1.5111e-02, -6.2461e-03,\n",
      "          8.2340e-03,  7.9926e-03, -4.6646e-03,  1.7921e-02, -6.8966e-03,\n",
      "          1.2530e-02,  2.6941e-03,  1.2757e-04,  2.2940e-02, -7.7927e-03,\n",
      "          2.0880e-03, -1.1081e-02,  1.0492e-02, -1.7854e-02, -9.0056e-03,\n",
      "         -2.8797e-03, -1.3808e-02, -1.5519e-02,  2.6725e-03, -7.4169e-03,\n",
      "          7.2371e-04, -8.0943e-03, -5.5009e-03,  2.9545e-03, -2.2277e-02,\n",
      "         -2.1909e-03, -2.7844e-03, -9.7359e-03, -6.9365e-03,  1.6697e-02,\n",
      "         -4.6375e-03, -8.7260e-03,  1.3164e-03,  1.7899e-02,  8.5343e-03,\n",
      "         -1.1839e-02, -1.2877e-02, -4.1906e-03,  1.4532e-02, -4.0053e-03,\n",
      "         -1.6149e-02, -5.3050e-03, -3.1532e-03,  4.9419e-03, -1.7574e-03,\n",
      "         -1.6074e-02,  1.1336e-02, -4.6015e-03, -1.4886e-02,  5.4531e-03,\n",
      "          5.8911e-03,  5.8006e-03, -6.7955e-03,  9.9125e-03, -7.7221e-04,\n",
      "          2.8354e-03, -5.7476e-03, -1.3386e-04, -2.4152e-02,  9.4562e-03,\n",
      "         -1.4748e-03, -4.6118e-03,  9.1050e-03, -8.1047e-03, -1.3755e-02,\n",
      "          4.7328e-03, -6.0818e-03, -1.9689e-02,  5.1948e-03, -6.4553e-03,\n",
      "          1.5456e-03,  2.4298e-03,  1.0547e-02, -7.7099e-03, -4.0233e-03,\n",
      "         -6.5864e-03,  3.6515e-03,  1.3893e-02,  1.2895e-03,  2.7441e-02,\n",
      "         -1.5326e-02, -1.2822e-02,  5.1758e-03,  3.0610e-03, -1.3291e-03,\n",
      "         -6.8926e-04, -3.1962e-03, -5.6259e-03, -1.3831e-02,  1.7329e-02,\n",
      "         -8.8763e-04, -2.2447e-02,  6.0220e-03,  1.0784e-02,  1.6179e-03,\n",
      "          3.3178e-03, -2.5439e-04, -5.7745e-03,  9.5628e-03,  1.0517e-02,\n",
      "         -5.9650e-03, -1.2241e-02, -9.8971e-03,  2.6634e-03,  6.2518e-03,\n",
      "          5.2850e-03,  1.6920e-02,  2.5247e-03, -1.0723e-02,  8.6159e-03,\n",
      "          6.7805e-03,  5.5439e-04,  1.0192e-02,  1.0224e-02,  3.0248e-03,\n",
      "          1.7879e-02, -1.3249e-02, -7.3367e-03, -4.8086e-03, -8.6301e-03,\n",
      "         -3.0151e-03,  2.2808e-03,  1.3188e-02,  8.1881e-03,  2.4232e-02,\n",
      "         -5.8235e-03,  1.3735e-03, -3.4911e-04, -2.8135e-03, -8.3579e-03,\n",
      "          1.3194e-02, -1.9868e-03, -4.5262e-03, -1.8044e-02,  5.2856e-03,\n",
      "         -1.1528e-03,  1.6673e-02,  6.5313e-03,  2.5311e-03, -1.4295e-02,\n",
      "          1.2721e-02,  1.2732e-02,  2.2533e-03,  1.9088e-03,  6.1749e-03,\n",
      "         -7.9723e-03, -3.3943e-03,  1.9464e-03,  3.9262e-03, -5.6412e-03,\n",
      "          1.7121e-02,  1.4594e-02,  1.0177e-02,  9.1464e-03, -1.6523e-04,\n",
      "          9.6971e-03,  1.6348e-02,  6.8789e-03, -1.7669e-02,  2.0441e-02,\n",
      "          1.4466e-03,  7.0307e-03, -9.4803e-03,  2.0991e-03, -6.8334e-03,\n",
      "          1.9979e-02,  3.5231e-03,  3.4146e-03, -1.9391e-02, -5.2830e-05,\n",
      "         -2.1146e-02, -1.1264e-02,  3.6875e-03, -1.2042e-02,  1.5938e-02,\n",
      "         -1.1722e-03,  4.8166e-03, -7.6629e-03,  1.5129e-02,  6.1675e-03,\n",
      "         -1.5512e-03,  5.9387e-04,  7.2354e-03,  1.3369e-02,  1.7735e-02,\n",
      "         -2.7782e-03, -9.1704e-03, -1.4664e-02, -1.0155e-02, -2.2111e-03,\n",
      "          3.6297e-02, -1.0760e-02,  7.3046e-03, -1.9801e-02,  1.3641e-02,\n",
      "          8.7312e-03,  4.0736e-03]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Targets tensor([[1.8044e-12, 1.7846e-12, 1.7676e-12, 1.7641e-12, 1.7601e-12, 1.7468e-12,\n",
      "         1.7552e-12, 1.7806e-12, 1.7908e-12, 1.7865e-12, 1.7836e-12, 1.7879e-12,\n",
      "         1.7968e-12, 1.8047e-12, 1.8194e-12, 1.8373e-12, 1.8377e-12, 1.8268e-12,\n",
      "         1.8126e-12, 1.8157e-12, 1.8313e-12, 1.8336e-12, 1.8400e-12, 1.8461e-12,\n",
      "         1.8383e-12, 1.8317e-12, 1.8350e-12, 1.8530e-12, 1.8786e-12, 1.8957e-12,\n",
      "         1.8949e-12, 1.8820e-12, 1.8890e-12, 1.8941e-12, 1.8831e-12, 1.8746e-12,\n",
      "         1.8778e-12, 1.8803e-12, 1.8653e-12, 1.8583e-12, 1.8582e-12, 1.8458e-12,\n",
      "         1.8471e-12, 1.8546e-12, 1.8447e-12, 1.8457e-12, 1.8478e-12, 1.8383e-12,\n",
      "         1.8470e-12, 1.8635e-12, 1.8700e-12, 1.8763e-12, 1.8709e-12, 1.8781e-12,\n",
      "         1.8932e-12, 1.8874e-12, 1.8880e-12, 1.8970e-12, 1.9038e-12, 1.9030e-12,\n",
      "         1.8908e-12, 1.8815e-12, 1.8832e-12, 1.8926e-12, 1.9084e-12, 1.9221e-12,\n",
      "         1.9238e-12, 1.9192e-12, 1.9098e-12, 1.8988e-12, 1.9059e-12, 1.9304e-12,\n",
      "         1.9405e-12, 1.9189e-12, 1.8875e-12, 1.8792e-12, 1.8821e-12, 1.8783e-12,\n",
      "         1.8716e-12, 1.8781e-12, 1.8996e-12, 1.8969e-12, 1.8696e-12, 1.8587e-12,\n",
      "         1.8529e-12, 1.8533e-12, 1.8678e-12, 1.8817e-12, 1.8739e-12, 1.8477e-12,\n",
      "         1.8367e-12, 1.8533e-12, 1.8871e-12, 1.9047e-12, 1.9083e-12, 1.9191e-12,\n",
      "         1.9252e-12, 1.9140e-12, 1.8960e-12, 1.9037e-12, 1.9211e-12, 1.9206e-12,\n",
      "         1.9172e-12, 1.9108e-12, 1.8985e-12, 1.8936e-12, 1.9000e-12, 1.9225e-12,\n",
      "         1.9411e-12, 1.9370e-12, 1.9329e-12, 1.9315e-12, 1.9245e-12, 1.9197e-12,\n",
      "         1.9180e-12, 1.9154e-12, 1.9115e-12, 1.9069e-12, 1.9026e-12, 1.8922e-12,\n",
      "         1.8772e-12, 1.8686e-12, 1.8644e-12, 1.8609e-12, 1.8616e-12, 1.8507e-12,\n",
      "         1.8314e-12, 1.8231e-12, 1.8233e-12, 1.8235e-12, 1.8169e-12, 1.8061e-12,\n",
      "         1.7963e-12, 1.7914e-12, 1.7879e-12, 1.7828e-12, 1.7814e-12, 1.7772e-12,\n",
      "         1.7686e-12, 1.7702e-12, 1.7718e-12, 1.7600e-12, 1.7474e-12, 1.7472e-12,\n",
      "         1.7514e-12, 1.7476e-12, 1.7329e-12, 1.7131e-12, 1.7030e-12, 1.7110e-12,\n",
      "         1.7167e-12, 1.7122e-12, 1.7124e-12, 1.7078e-12, 1.6867e-12, 1.6805e-12,\n",
      "         1.7045e-12, 1.7265e-12, 1.7349e-12, 1.7403e-12, 1.7468e-12, 1.7389e-12,\n",
      "         1.7251e-12, 1.7181e-12, 1.7113e-12, 1.7101e-12, 1.7111e-12, 1.7033e-12,\n",
      "         1.6976e-12, 1.6922e-12, 1.6764e-12, 1.6478e-12, 1.6324e-12, 1.6363e-12,\n",
      "         1.6328e-12, 1.6247e-12, 1.6172e-12, 1.6101e-12, 1.6114e-12, 1.6213e-12,\n",
      "         1.6300e-12, 1.6289e-12, 1.6208e-12, 1.6056e-12, 1.5941e-12, 1.5965e-12,\n",
      "         1.6000e-12, 1.5971e-12, 1.5995e-12, 1.5950e-12, 1.5851e-12, 1.5832e-12,\n",
      "         1.5835e-12, 1.5830e-12, 1.5850e-12, 1.5921e-12, 1.5915e-12, 1.5847e-12,\n",
      "         1.5773e-12, 1.5801e-12, 1.5887e-12, 1.5992e-12, 1.6050e-12, 1.5982e-12,\n",
      "         1.5899e-12, 1.5862e-12, 1.5831e-12, 1.5965e-12, 1.6176e-12, 1.6245e-12,\n",
      "         1.6192e-12, 1.6143e-12, 1.6158e-12, 1.6107e-12, 1.6088e-12, 1.6120e-12,\n",
      "         1.6190e-12, 1.6271e-12, 1.6267e-12, 1.6138e-12, 1.5885e-12, 1.5690e-12,\n",
      "         1.5646e-12, 1.5654e-12, 1.5702e-12, 1.5658e-12, 1.5547e-12, 1.5511e-12,\n",
      "         1.5560e-12, 1.5598e-12, 1.5532e-12, 1.5485e-12, 1.5471e-12, 1.5424e-12,\n",
      "         1.5229e-12, 1.5042e-12, 1.4990e-12, 1.5008e-12, 1.5129e-12, 1.5226e-12,\n",
      "         1.5255e-12, 1.5250e-12, 1.5180e-12, 1.5179e-12, 1.5234e-12, 1.5303e-12,\n",
      "         1.5430e-12, 1.5482e-12, 1.5548e-12, 1.5682e-12, 1.5721e-12, 1.5672e-12,\n",
      "         1.5564e-12, 1.5510e-12, 1.5510e-12, 1.5419e-12, 1.5372e-12, 1.5452e-12,\n",
      "         1.5517e-12, 1.5559e-12, 1.5583e-12, 1.5562e-12, 1.5437e-12, 1.5272e-12,\n",
      "         1.5220e-12, 1.5163e-12, 1.5050e-12, 1.4865e-12, 1.4734e-12, 1.4737e-12,\n",
      "         1.4706e-12, 1.4590e-12, 1.4438e-12, 1.4418e-12, 1.4442e-12, 1.4437e-12,\n",
      "         1.4456e-12, 1.4409e-12, 1.4353e-12, 1.4285e-12, 1.4142e-12, 1.4128e-12,\n",
      "         1.4257e-12, 1.4293e-12, 1.4169e-12, 1.4136e-12, 1.4185e-12, 1.4184e-12,\n",
      "         1.4218e-12, 1.4386e-12, 1.4449e-12, 1.4364e-12, 1.4368e-12, 1.4379e-12,\n",
      "         1.4304e-12, 1.4296e-12, 1.4282e-12, 1.4273e-12, 1.4473e-12, 1.4671e-12,\n",
      "         1.4641e-12, 1.4600e-12, 1.4773e-12, 1.4943e-12, 1.4971e-12, 1.5011e-12,\n",
      "         1.5067e-12, 1.5073e-12, 1.5098e-12, 1.5166e-12, 1.5228e-12, 1.5243e-12,\n",
      "         1.5258e-12, 1.5249e-12, 1.5233e-12, 1.5267e-12, 1.5195e-12, 1.5055e-12,\n",
      "         1.5042e-12, 1.5002e-12, 1.4954e-12, 1.4981e-12, 1.4945e-12, 1.4861e-12,\n",
      "         1.4815e-12, 1.4780e-12, 1.4682e-12, 1.4642e-12, 1.4669e-12, 1.4640e-12,\n",
      "         1.4592e-12, 1.4641e-12, 1.4734e-12, 1.4794e-12, 1.4875e-12, 1.4927e-12,\n",
      "         1.4893e-12, 1.4922e-12, 1.4992e-12, 1.5026e-12, 1.5095e-12, 1.5179e-12,\n",
      "         1.5177e-12, 1.5113e-12, 1.5182e-12, 1.5322e-12, 1.5370e-12, 1.5429e-12,\n",
      "         1.5587e-12, 1.5694e-12, 1.5709e-12, 1.5733e-12, 1.5717e-12, 1.5744e-12,\n",
      "         1.5853e-12, 1.5954e-12, 1.5975e-12, 1.5884e-12, 1.5830e-12, 1.5815e-12,\n",
      "         1.5784e-12, 1.5778e-12, 1.5841e-12, 1.5885e-12, 1.5839e-12, 1.5815e-12,\n",
      "         1.5746e-12, 1.5650e-12, 1.5686e-12, 1.5734e-12, 1.5828e-12, 1.5875e-12,\n",
      "         1.5858e-12, 1.5757e-12, 1.5623e-12, 1.5575e-12, 1.5573e-12, 1.5670e-12,\n",
      "         1.5751e-12, 1.5691e-12, 1.5621e-12, 1.5600e-12, 1.5664e-12, 1.5837e-12,\n",
      "         1.6111e-12, 1.6317e-12, 1.6390e-12, 1.6516e-12, 1.6588e-12, 1.6622e-12,\n",
      "         1.6732e-12, 1.7073e-12, 1.7550e-12, 1.7859e-12, 1.8045e-12, 1.8398e-12,\n",
      "         1.8667e-12, 1.8800e-12, 1.8920e-12, 1.9138e-12, 1.9296e-12, 1.9120e-12,\n",
      "         1.8945e-12, 1.9073e-12, 1.9148e-12, 1.9196e-12, 1.9354e-12, 1.9518e-12,\n",
      "         1.9625e-12, 1.9544e-12, 1.9573e-12, 1.9611e-12, 1.9567e-12, 1.9397e-12,\n",
      "         1.9135e-12, 1.9121e-12, 1.9089e-12, 1.8928e-12, 1.8917e-12, 1.9098e-12,\n",
      "         1.9291e-12, 1.9251e-12, 1.9175e-12, 1.9149e-12, 1.9127e-12, 1.9239e-12,\n",
      "         1.9116e-12, 1.9287e-12, 1.9684e-12, 1.9887e-12, 2.0033e-12, 1.9992e-12],\n",
      "        [3.6922e-13, 3.6684e-13, 3.6274e-13, 3.6256e-13, 3.6812e-13, 3.8025e-13,\n",
      "         3.8038e-13, 3.8031e-13, 3.7914e-13, 3.7201e-13, 3.6717e-13, 3.6543e-13,\n",
      "         3.5997e-13, 3.5548e-13, 3.3659e-13, 3.3460e-13, 3.4074e-13, 3.4621e-13,\n",
      "         3.5028e-13, 3.5243e-13, 3.5047e-13, 3.5301e-13, 3.5193e-13, 3.4764e-13,\n",
      "         3.4539e-13, 3.4759e-13, 3.4358e-13, 3.4274e-13, 3.3567e-13, 3.3698e-13,\n",
      "         3.3816e-13, 3.3795e-13, 3.3692e-13, 3.3088e-13, 3.3012e-13, 3.3368e-13,\n",
      "         3.3471e-13, 3.3488e-13, 3.3046e-13, 3.2368e-13, 3.2359e-13, 3.2440e-13,\n",
      "         3.2335e-13, 3.2393e-13, 3.1872e-13, 3.1429e-13, 3.1370e-13, 3.1903e-13,\n",
      "         3.1865e-13, 3.1892e-13, 3.1517e-13, 3.1481e-13, 3.1243e-13, 3.0686e-13,\n",
      "         3.0111e-13, 2.9736e-13, 2.9751e-13, 2.9676e-13, 2.9770e-13, 2.9688e-13,\n",
      "         2.9389e-13, 2.9258e-13, 2.8314e-13, 2.7198e-13, 2.6622e-13, 2.6505e-13,\n",
      "         2.6123e-13, 2.5901e-13, 2.6012e-13, 2.6141e-13, 2.5971e-13, 2.5585e-13,\n",
      "         2.5589e-13, 2.5860e-13, 2.5730e-13, 2.5442e-13, 2.5392e-13, 2.5234e-13,\n",
      "         2.5240e-13, 2.5235e-13, 2.5012e-13, 2.4995e-13, 2.4448e-13, 2.4168e-13,\n",
      "         2.4114e-13, 2.4283e-13, 2.4318e-13, 2.4318e-13, 2.4466e-13, 2.4402e-13,\n",
      "         2.3847e-13, 2.3293e-13, 2.3760e-13, 2.3639e-13, 2.3830e-13, 2.3782e-13,\n",
      "         2.3782e-13, 2.3621e-13, 2.3680e-13, 2.3604e-13, 2.3613e-13, 2.3106e-13,\n",
      "         2.3032e-13, 2.3172e-13, 2.3108e-13, 2.2916e-13, 2.2791e-13, 2.2680e-13,\n",
      "         2.2805e-13, 2.3178e-13, 2.3036e-13, 2.2781e-13, 2.2892e-13, 2.2838e-13,\n",
      "         2.2841e-13, 2.2909e-13, 2.2994e-13, 2.3039e-13, 2.3070e-13, 2.2965e-13,\n",
      "         2.3158e-13, 2.3157e-13, 2.3172e-13, 2.2831e-13, 2.2480e-13, 2.2178e-13,\n",
      "         2.2316e-13, 2.2193e-13, 2.1945e-13, 2.1916e-13, 2.1721e-13, 2.1270e-13,\n",
      "         2.1292e-13, 2.1082e-13, 2.0968e-13, 2.0740e-13, 2.0944e-13, 2.1180e-13,\n",
      "         2.1422e-13, 2.2058e-13, 2.2258e-13, 2.2414e-13, 2.2460e-13, 2.2470e-13,\n",
      "         2.2473e-13, 2.2312e-13, 2.2237e-13, 2.2437e-13, 2.2377e-13, 2.2293e-13,\n",
      "         2.2021e-13, 2.1875e-13, 2.1751e-13, 2.1658e-13, 2.1385e-13, 2.1302e-13,\n",
      "         2.1134e-13, 2.1002e-13, 2.0809e-13, 2.0675e-13, 2.0477e-13, 2.0440e-13,\n",
      "         2.0488e-13, 2.0375e-13, 2.0071e-13, 1.9861e-13, 1.9843e-13, 1.9755e-13,\n",
      "         2.0037e-13, 2.0139e-13, 2.0514e-13, 2.0575e-13, 2.0743e-13, 2.0820e-13,\n",
      "         2.0853e-13, 2.0819e-13, 2.0898e-13, 2.0881e-13, 2.0786e-13, 2.0752e-13,\n",
      "         2.0816e-13, 2.0861e-13, 2.0871e-13, 2.0903e-13, 2.0717e-13, 2.0692e-13,\n",
      "         2.0778e-13, 2.0606e-13, 2.0319e-13, 2.0114e-13, 2.0104e-13, 1.9960e-13,\n",
      "         1.9990e-13, 2.0304e-13, 2.0147e-13, 2.0300e-13, 2.0197e-13, 2.0571e-13,\n",
      "         2.0787e-13, 2.0674e-13, 2.0484e-13, 2.0425e-13, 2.0219e-13, 1.9865e-13,\n",
      "         1.9675e-13, 1.9608e-13, 1.9593e-13, 1.9345e-13, 1.9262e-13, 1.9397e-13,\n",
      "         1.9422e-13, 1.9388e-13, 1.9367e-13, 1.9174e-13, 1.9210e-13, 1.9299e-13,\n",
      "         1.9399e-13, 1.9773e-13, 1.9755e-13, 1.9847e-13, 1.9858e-13, 1.9612e-13,\n",
      "         1.9810e-13, 1.9930e-13, 1.9891e-13, 1.9754e-13, 1.9844e-13, 1.9879e-13,\n",
      "         1.9813e-13, 1.9769e-13, 1.9909e-13, 2.0077e-13, 2.0068e-13, 2.0004e-13,\n",
      "         2.0370e-13, 2.0606e-13, 2.0559e-13, 2.0790e-13, 2.0684e-13, 2.0715e-13,\n",
      "         2.0686e-13, 2.0435e-13, 2.0423e-13, 2.0692e-13, 2.0391e-13, 2.0498e-13,\n",
      "         2.0488e-13, 2.0602e-13, 2.0607e-13, 2.0494e-13, 2.0387e-13, 2.0187e-13,\n",
      "         1.9921e-13, 1.9918e-13, 1.9823e-13, 1.9770e-13, 1.9830e-13, 1.9963e-13,\n",
      "         1.9936e-13, 2.0077e-13, 2.0081e-13, 1.9931e-13, 1.9827e-13, 2.0012e-13,\n",
      "         1.9916e-13, 1.9626e-13, 1.9552e-13, 1.9641e-13, 1.9845e-13, 1.9773e-13,\n",
      "         1.9749e-13, 1.9696e-13, 1.9721e-13, 1.9769e-13, 1.9707e-13, 1.9433e-13,\n",
      "         1.9300e-13, 1.9237e-13, 1.9193e-13, 1.9134e-13, 1.9289e-13, 1.9316e-13,\n",
      "         1.9319e-13, 1.9396e-13, 1.9436e-13, 1.9514e-13, 1.9597e-13, 1.9919e-13,\n",
      "         2.0027e-13, 2.0120e-13, 1.9932e-13, 1.9788e-13, 2.0095e-13, 2.0054e-13,\n",
      "         2.0030e-13, 1.9926e-13, 1.9630e-13, 1.9840e-13, 1.9938e-13, 1.9843e-13,\n",
      "         1.9542e-13, 1.9293e-13, 1.9160e-13, 1.9220e-13, 1.9028e-13, 1.9008e-13,\n",
      "         1.8682e-13, 1.8710e-13, 1.8875e-13, 1.8845e-13, 1.8702e-13, 1.8914e-13,\n",
      "         1.9077e-13, 1.9139e-13, 1.9119e-13, 1.9033e-13, 1.8677e-13, 1.9038e-13,\n",
      "         1.9297e-13, 1.9403e-13, 1.9055e-13, 1.9059e-13, 1.9252e-13, 1.9346e-13,\n",
      "         1.9497e-13, 1.9753e-13, 1.9758e-13, 1.9629e-13, 1.9822e-13, 1.9923e-13,\n",
      "         2.0075e-13, 2.0113e-13, 2.0126e-13, 2.0176e-13, 2.0203e-13, 2.0045e-13,\n",
      "         2.0102e-13, 2.0009e-13, 2.0005e-13, 2.0014e-13, 1.9925e-13, 1.9798e-13,\n",
      "         1.9745e-13, 1.9453e-13, 1.9213e-13, 1.9023e-13, 1.8975e-13, 1.9022e-13,\n",
      "         1.8947e-13, 1.8854e-13, 1.8739e-13, 1.8689e-13, 1.8720e-13, 1.8564e-13,\n",
      "         1.8301e-13, 1.8287e-13, 1.8334e-13, 1.8649e-13, 1.8553e-13, 1.8616e-13,\n",
      "         1.8689e-13, 1.8772e-13, 1.8740e-13, 1.9028e-13, 1.9093e-13, 1.8940e-13,\n",
      "         1.8873e-13, 1.9055e-13, 1.9122e-13, 1.9134e-13, 1.9375e-13, 1.9524e-13,\n",
      "         1.9567e-13, 1.9327e-13, 1.9347e-13, 1.9185e-13, 1.9135e-13, 1.9064e-13,\n",
      "         1.9094e-13, 1.8954e-13, 1.8966e-13, 1.8821e-13, 1.8516e-13, 1.8784e-13,\n",
      "         1.9094e-13, 1.9200e-13, 1.9125e-13, 1.9260e-13, 1.9235e-13, 1.9087e-13,\n",
      "         1.8870e-13, 1.9057e-13, 1.9234e-13, 1.8984e-13, 1.8783e-13, 1.8903e-13,\n",
      "         1.8787e-13, 1.8871e-13, 1.9019e-13, 1.8999e-13, 1.8964e-13, 1.8903e-13,\n",
      "         1.8658e-13, 1.8705e-13, 1.8769e-13, 1.8725e-13, 1.8529e-13, 1.8501e-13,\n",
      "         1.8582e-13, 1.8639e-13, 1.8649e-13, 1.9016e-13, 1.9086e-13, 1.9229e-13,\n",
      "         1.9105e-13, 1.8999e-13, 1.8694e-13, 1.8292e-13, 1.8237e-13, 1.8246e-13,\n",
      "         1.8151e-13, 1.8143e-13, 1.8123e-13, 1.8092e-13, 1.8217e-13, 1.8310e-13,\n",
      "         1.8348e-13, 1.8231e-13, 1.8244e-13, 1.8311e-13, 1.8140e-13, 1.8035e-13]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Eval Batch 1/51 — OMNI2 Mask Sum: 656640.0 | GOES Mask Sum: 6515662.0 | Density Mask Sum: 3443.0\n",
      "🧪 Eval Batch 16/51 — OMNI2 Mask Sum: 656640.0 | GOES Mask Sum: 4162416.0 | Density Mask Sum: 3343.0\n",
      "🧪 Eval Batch 22/51 — OMNI2 Mask Sum: 656640.0 | GOES Mask Sum: 5345290.0 | Density Mask Sum: 3416.0\n",
      "🧪 Eval Batch 49/51 — OMNI2 Mask Sum: 656640.0 | GOES Mask Sum: 6166782.0 | Density Mask Sum: 3456.0\n",
      "🧪 Eval Batch 51/51 — OMNI2 Mask Sum: 492480.0 | GOES Mask Sum: 2937816.0 | Density Mask Sum: 2592.0\n",
      "\n",
      "📊 Epoch 1/100 — Train Loss: 0.06857383849207482 | Val Loss: 0.001162332117411436\n",
      "Preds: tensor([[ 0.0200,  0.0170,  0.0061,  ...,  0.0204,  0.0022,  0.0330],\n",
      "        [ 0.0086,  0.0257, -0.0267,  ...,  0.0172, -0.0041,  0.0142],\n",
      "        [-0.0064,  0.0110, -0.0029,  ...,  0.0101, -0.0230, -0.0234],\n",
      "        [ 0.0004,  0.0095, -0.0073,  ..., -0.0118, -0.0027,  0.0012],\n",
      "        [ 0.0176,  0.0177, -0.0025,  ...,  0.0288, -0.0060,  0.0338],\n",
      "        [-0.0102,  0.0165,  0.0005,  ...,  0.0052, -0.0245, -0.0228]],\n",
      "       device='cuda:0')\n",
      "Targets tensor([[4.4862e-14, 4.4614e-14, 4.4473e-14,  ..., 4.3594e-14, 4.3997e-14,\n",
      "         4.4936e-14],\n",
      "        [2.5604e-13, 2.5200e-13, 2.4636e-13,  ..., 1.3770e-13, 1.4635e-13,\n",
      "         1.5376e-13],\n",
      "        [5.1611e-12, 5.1743e-12, 5.1893e-12,  ..., 4.7765e-12, 4.7810e-12,\n",
      "         4.7847e-12],\n",
      "        [1.1828e-12, 1.1794e-12, 1.1814e-12,  ..., 8.5150e-13, 8.3358e-13,\n",
      "         8.3713e-13],\n",
      "        [1.5139e-13, 1.4443e-13, 1.3670e-13,  ..., 1.4753e-13, 1.5153e-13,\n",
      "         1.4713e-13],\n",
      "        [5.6139e-12, 5.6438e-12, 5.6751e-12,  ..., 5.9039e-12, 5.8779e-12,\n",
      "         5.8388e-12]], device='cuda:0')\n",
      "✅ Best model updated.\n",
      "\n",
      "🚀 Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 30/964 [00:24<12:54,  1.21it/s] \n",
      "Exception in thread Thread-29 (_pin_memory_loop):\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/runpod_conda/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/runpod_conda/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/root/miniconda3/envs/runpod_conda/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/runpod_conda/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py\", line 59, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/root/miniconda3/envs/runpod_conda/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py\", line 35, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/envs/runpod_conda/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/envs/runpod_conda/lib/python3.12/site-packages/torch/multiprocessing/reductions.py\", line 541, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/envs/runpod_conda/lib/python3.12/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/envs/runpod_conda/lib/python3.12/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/envs/runpod_conda/lib/python3.12/multiprocessing/connection.py\", line 519, in Client\n",
      "    c = SocketClient(address)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/envs/runpod_conda/lib/python3.12/multiprocessing/connection.py\", line 647, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_storm_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_states_normalized\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mtrain_storm_transformer\u001b[39m\u001b[34m(initial_states_df, num_epochs, batch_size, lr, device)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🚀 Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m#start_load = time.time()\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstatic_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdensity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdensity_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgoes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgoes_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43momni2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43momni2_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#nd_load = time.time()\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/runpod_conda/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/runpod_conda/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/runpod_conda/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1458\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data)\n\u001b[32m   1457\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1458\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1459\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1461\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/runpod_conda/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1410\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1408\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1409\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1410\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1411\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1412\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/runpod_conda/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1251\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1239\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1240\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1248\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1250\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1254\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1255\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1256\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/runpod_conda/lib/python3.12/queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/runpod_conda/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_storm_transformer(initial_states_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05500b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe5d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "omni2_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "runpod_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
